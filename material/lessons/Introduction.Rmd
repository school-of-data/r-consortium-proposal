
```{r, echo=FALSE}
knitr::opts_chunk$set(message = FALSE, cache = TRUE)
```


# Introductory steps - phase 1

The introductory steps are a guided process through which:

* we spell out clearly how R can be used for a basic data story
* we introduce the reader to the basics of R, iterating on each notion learned in the previous step
* we contextualise the step within a data-driven story writing workflow.

## Main intro
_interactive quiz used as a way to start engaging the visitor right away, and to get statistics from various populations_

    Are you a journalist?      
    -> Yes -> No      
    
    |____(if no) this tutorial is delibarately, intensely, exclusively focused on journalists. If you want to continue you'll have to pretend to be a journalist and, believe us, it's not pleasant. Do you still want to continue?
    
    ____ -> Yes -> No     
    
         |     
         |____ (if yes) Have you ever heard about this thing we call R?     
    
         ____-> Yes -> No   
             |     
             |____ (if no) Well, despite its mysterious and concise name, R is an open source programming 
             language and environment for statistical computing, widely used among data analysts, statisticians, 
             programmers and more recently, by data journalists.   Our mission today will be to convince you
             that R is worth your precious time and will help you improve your work in the newsroom. 
             
             |     
             |____ (if yes) Do you already use R your work?     
    
              ____-> Yes -> No   
    
                    |    
                    |____________ (if yes) What follows is a beginner tutorial for R-newbies. 
                    You could jump directly to [our list of R recipes]() or still follow the introduction tutorial 
                    to steal ideas for your own R trainings (we know who you are!).
    
                    |     
                    |____________ (if no) So an R atheist? Or maybe agnostic? 
                    Our mission will be to convice you to actually start using R in your workflow. 
                    If we fail, we allow you the privilege to send us an angry tweet.

      --------------------     
     | Challenge accepted |    
      --------------------    

## The context
_Here we set up the story that will be used to contextualise all actions in the tutorial_

A referendum in your country is over and you are tasked to do a post-election analysis. You have data about the number of votes, demographics and geographic distribution of the votes.  In order to see what would make a good story, you decide to analyse and visualise the data.
Even though you normally work with spreadsheets, you want to try R.  Here are a few reasons why:

Excel | R 
-------------------------------- | --------------------------------
It's a "point and click program" | You write a functions directly in the R interface and R does all the magic
If you want to repeat a process you have to do it every time | R allows for automation of processes that have to be repeated. Write once, reuse several times: Not only can you update your code easily, but you can easily share it with other projects.
Only allows some types of files | Can load different kinds of files that can be complicated or impossible to work with in Excel
.... | .....

> **Wait, what?**
>   
> how do I even know how write R code? I'm not a programmer! 
>    
> Well you don't need to: as long as you understand what you want to do, you will probably find an answer in the [documentation](), [tutorials]() or [existing projects]().

We are going to guide you in each step, don't worry, it won't be that hard. Let's start with getting the data.

## Getting the data

You stretch your arms, grab a coffee, and off you go! First, you have to find the electoral data. For this example, we are going to analyze the Brexit Referendum Results.  The results data, compiled by the Electoral Comission are available [here](https://github.com/school-of-data/r-consortium-proposal/blob/master/material/lessons/results.csv) 
in .csv format 

You also want to align it with a dataset which gives various demographic information about regions, in order to try various analyses. We are going to use 2 files: a .dta file (a filetype that you can not open in Excel) that is available [here](https://github.com/school-of-data/r-consortium-proposal/blob/master/material/lessons/demographics2.dta)  and a .xlsx file available [here](https://github.com/school-of-data/r-consortium-proposal/blob/master/material/lessons/demographics1.xlsx). 

Metadata: 

_Results_

- `id` Unique id
- `Region_Code` Region Code
- `Region` Region Name                 
- `area_code` Unique area code
- `Area` Area Name
- `Electorate` Number of registered voters             
- `ExpectedBallots` Expected ballots
- `VerifiedBallotPapers` Verified Ballots
- `Pct_Turnout` Percentage of voter turnout.  VerifiedBallotPapers/Electorate
- `Votes_Cast` Casted Votes
- `Valid_Votes` Number of Valid votes
- `Remain` Number of "Remain" votes                
- `Leave` Number of "Leave" votes                   
- `Rejected_Ballots` Number of rejected ballots

_Demographics1_

- `new_id` unique Id
- `area_code` Unique area code
- `area_name` Area Name
- `median_age` Median age of residents
- `prof_ocu_1`  Percentage of residents with higher and intermediate professional occupations 
- `prof_ocu_2`  Percentage of residents with junior professional occupations 

_Demographics2_

- `new_id` unique Id
- `area_code` Unique area code
- `area_name` Area Name
- `born_uk` Percentage of residents born in the UK
- `no_educ` Percentage of residents with no formal education
- `higher_educ` Percentage of residents with higher education

Let's download those 3 files and store them in a folder called `electoral-data`.
You must have 3 files in your folder: 
`results.cvs`
`demographics1.xlsx`
`demographics2.dta`

Now let's try reading in the data:
1. Open your R editor (e.g. RStudio)
2. Set your working directory to `electoral-data`. To do this enter
```{r, eval=FALSE}
setwd("/PATH/electoral-data")
```
in the console.
Then, we are going to need two specialised packages to read .dta and .xlsx files into R: `xlsx` and `foreign`.
To install the packages run
```{r, eval=FALSE}
install.packages("xlsx")
install.packages("foreign")
```
To load the packages run
```{r}
library("xlsx") 
library("foreign") 
```
Now we can read the data: 
```{r}
# read results data
results <- read.csv("results.csv", header = TRUE)

# read demographic data
demographic1 <- read.xlsx("demographics1.xlsx", sheetIndex = 1)
demographic2 <- read.dta("demographics2.dta")
```
(Note the the hash: it can be used to add comments. You should use it to help future you remember what you did.)

See? That was easy! Now you have your data loaded into the program and can start working.  

Are you seeing the magic yet? 

* A simple line of code can replace cumbersome manual operations that you have to do in Excel to import the data
* R loads all sort of file formats, and doesn't complain
* Write once, reuse several times: Not only you can update your code easily, but you can easily share it with other projects.
* Your stagiaire doesn't know where to find the relevant data? no problem, the links are all in the R code!

> **Let's review the commands we learned in this step:**
>
- Set the folder you work in (working directory) `setwd()` 
- Install Packages `install.packages()`
- Load Libraries `library()`
- Read files `read.csv` `read.xlsx` `read.dta`
- Add code comments `# my comment`


> **Want to learn what other types of data can be imported into R**
>   
> [Check this!](https://www.datacamp.com/community/tutorials/r-data-import-tutorial#gs.oZSkvFo)
>    

Now let's move on with the analysis!

## Verifying the data
Now that you have imported your data into R, it's time to start exploring it.  Here are some useful commands:

- `view()` shows your dataframe with all the variables and observations

- `names()` shows the collumn names of the data frame
```{r}
names(results)
names(demographic1)
names(demographic2)
```
- `str()` shows the type of each variable (here `num` for numeric and `chr` for character) and the
first few values
```{r}
str(results)
```

Now that you know the variables of your databases is time to start with the cleaning part.
    

## Cleaning the data
Before you can start your analysis, you need a clean database with all the information.  Nevertheless you have the information scattered in 3 different data.frames, which you would like to combine.  

Our objective is to add the demographic variables `median_age` `prof_ocu_1` `prof_ocu_2` `born_uk` `no_educ` and `higher_educ` to the results database.  But in order to do this, we need a common variable in all the datasets.  In the previous step, where we explored the data you can see that the three databases have the variable: `area_code`.  

That common variable will allow us to merge the databases using the `merge` function.

Simply type:
```{r}
## merge the data sets
mydata <- merge(results, demographic1, by.x = "Area_Code", by.y = "area_code")
mydata <- merge(mydata, demographic2, by.x = "Area_Code", by.y = "area_code")

## list all variables in mydata
names(mydata)
```
You now have a new dataframe `mydata` with the electoral and the demographic information. Nevertheless we need to further clean the database.  

<!-- HS: Do we need to delete the variables? If, so I would use package dplyr, function select. -->

<!-- First, we are going to delete the variables that we don't need.  To do this we just type the name of our dataframe followed by $ and the variable we want to delete, like this: `mydata$variable <- NULL`. -->

<!-- ```{r} -->
<!-- ##Delete the variables we don't need -->
<!-- mydata$new_id.x <- NULL -->
<!-- mydata$area_name.x <- NULL -->
<!-- mydata$new_id.y <- NULL -->
<!-- mydata$area_name.y <- NULL -->
<!-- ``` -->
To analyze the data we also need the percentage of Remain, Leave and Rejected votes for each area.  We are going to create those three variables:
```{r}
mydata$perc_remain <- (mydata$Remain / mydata$Valid_Votes) * 100
mydata$perc_leave <- (mydata$Leave / mydata$Valid_Votes) * 100
mydata$perc_rejected <- (mydata$Rejected_Ballots / mydata$Votes_Cast) * 100
```
Also we want to sum `prof_ocu_1` and `prof_ocu_2` to create a new variable that contains the percentage of residents in each area with professional occupations:
```{r}
mydata$prof_ocu <- mydata$prof_ocu_1 + mydata$prof_ocu_2
```
We can also create a new categorical variable `Result` that shows which areas voted to leave and which voted to remain. To do this we are going to use the `ifelse()` function. To look at the number of areas who voted for and against remaining we can use the `table()` function.
```{r} 
## This fuction creates a new variable that asigns the value 1 if the remain percentage is less than 50, 
## and 0 otherwise
mydata$result <- ifelse(mydata$perc_remain < 50, 1, 0)

## Table of the result
table(mydata$result)
```
Now we can add labels to the variable `Result`
```{r} 
mydata$result <- factor(mydata$result,
                        levels = c(1, 0),
                        labels = c("leave", "remain"))

## Table of the result
table(mydata$result)
```

We can also create a new categorical variable that allows to group the Remain vote in different categories:
```{r} 
## create 4 categories of voting percentages
mydata$Remain_cat <- cut(mydata$perc_remain, breaks = c(0, 25, 50, 75, 100))
table(mydata$Remain_cat)
```
Now we can add labels to the variable `Remain_cat`
```{r} 
mydata$Remain_cat <- factor(mydata$Remain_cat, labels = c("very low", "low", "high", "very high"))
table(mydata$Remain_cat)
```

We are almost done, don't give up just yet! Now to finish we want to aggregate regional data in new varaibles, in order the see the voting results by region. A simple way to do this is with the `summarise()` function from the `dplyr` package. If you don't have the package installed, install it with the command `install.packages("dplyr")`.

```{r}
library("dplyr")
datahasc <- summarise(group_by(mydata, HASC_code),        ## Group mydata by HASC code
                        Valid_Votes = sum(Valid_Votes),  ## Create a variable Valid_Votes which is the number of valid votes per HASC code, i.e. the sum of valid votes in the areas of the HASC code
                        Remain = sum(Remain), 
                        Leave = sum(Leave))

## Calculate the percentages
datahasc$perc_remain <- (datahasc$Remain / datahasc$Valid_Votes)*100
datahasc$perc_leave <- (datahasc$Leave/ datahasc$Valid_Votes)*100

datahasc
```


<!-- ```{r}  -->
<!-- library("dplyr") -->
<!-- dataregion <- summarise(group_by(mydata, Region),        ## Group mydata by variable Region -->
<!--                         Valid_Votes = sum(Valid_Votes),  ## Create a variable Valid_Votes which is the number of valid votes per region, i.e. the sum of valid votes in the areas of the region -->
<!--                         Remain = sum(Remain),  -->
<!--                         Leave = sum(Leave)) -->

<!-- ## Calculate the percentages -->
<!-- dataregion$perc_remain <- (dataregion$Remain / dataregion$Valid_Votes)*100 -->
<!-- dataregion$perc_leave <- (dataregion$Leave/ dataregion$Valid_Votes)*100 -->

<!-- dataregion -->
<!-- ``` -->
<!-- _Heidi: Do you think we can do somehing else here in cleaning, to introduce a package, maybe filter data?_ -->
<!-- I think we should keep it short, so I would leave it as is. -->

Now that you have cleaned your data, you're ready to start the analysis! 

**Are you convinced yet that R is worth your time?**     
____ not really    
____ hmmm, I need to see more to decide     
____ yes, R won my heart  

**Let's list some advantages of R so far:** 
* With simple lines of code you can rearrange your data, create new variables, delete observations
* Something went wrong? Don't worry, just fix your code, run it and the error goes away easily
* You can keep your original datafiles in the workspace if you need to review them.  

> **Let's review what we learned in this step:**
>
- Merge datasets `merge()` 
- Create new variables
<!-- - Delete variables -->
- Create new categorical variables and asign labels
- Create a new data set with summaries of an existing data set 


## Analyze
Let's start with the most fun part. You need to write a story based on those electoral results, and you already have some questions 
you want to answer:
- Which areas voted to remain or to leave?
- Which regions? 
- Which were the more divided regions?
- How do the demographic variables correlate with the leave and remain percentage of votes?

First we need to tabulate the data:
```{r}
## How many areas voted to remain or leave
table(mydata$result)
prop.table(table(mydata$result))
```
You can see that 69% of the areas voted to leave the European Union.  Now let's see how are they are distributed by Region creating a two frequency table that shows row percentages.  To do that we are going to use the `CrossTab()` function in the `gmodels` package (rember to install it with `install.packages("gmodels")`):
```{r}
library("gmodels") 
CrossTable(mydata$Region, mydata$result, digits=3, max.width = 5, expected = FALSE, 
           prop.r = TRUE, prop.c = FALSE, prop.t = FALSE, prop.chisq = FALSE)

# Note that we set the prop.r = TRUE in order to show row percetages.  
# If we wanted for instance to show column percentages
# we can set prop.c = TRUE 
```
With that table you can see that the only regions in wich the majority of areas voted to remain in the EU are located in London, Scotland
and Northern Ireland.  This could be interesting for your story!

We can also calculate some summary statistics for the `perc_remain` variable:
```{r}
summary(mydata$perc_remain)
table(mydata$Remain_cat)
```
You can see that only one area had a very low vote to Remain with 24.4% of the vote.

We can also tabulate the 10 areas with the highest Leave vote:
```{r}
## order data by percentage of leave votes
order_leave <- order(mydata$perc_leave, decreasing = TRUE)
mydata <- mydata[order_leave, ]

## show the first 10 observations, show only some of the variables
mydata[1:10, c("Region", "Area", "Leave", "Remain", "perc_leave")]
```
So far you have answered some of your questions: the majority of areas in the UK voted to leave the EU.  London, Scotland and Northern Ireland voted by majority to Remain.  

So it's time to go deeper in the analysis.  We can calculate some correlations between the leave vote and some of the demographic variables.  The hyphotesis is that the regions with older population, less educated residents and less diverse voted to Leave the EU.  

Now let's calculate some correlations.  To calculate a correlation between two variables just use the fuction `cor()` and plot it using `plot(x, y)`:
```{r}
cor(mydata$perc_leave, mydata$no_educ)
plot(perc_leave ~ no_educ, data = mydata)
```


To see if our correlation is statistically significant we can use a linear model:
```{r}
### look at Pr(<|t|) in the no_educ row. The p-value is very small 
### => statistically significant linear relation between no_educ and perc_leave
model <- lm(perc_leave ~ no_educ, data = mydata)
summary(model)

### visualise the estimated relation
plot(perc_leave ~ no_educ, data = mydata)
abline(model)
```
_Do we even want to go into stats so deeply?_


To calculate a correlation matrix, let's create a new dataframe only with the varaibles we need and then use the `cor()` function again:
```{r}
datacor <- select(.data = mydata, 
                  perc_leave, perc_remain, median_age, born_uk, no_educ, higher_educ, prof_ocu)

#Lets calculate the correlation matrix
cor(datacor)
```
You can see that there's a positive correlation between the Remain vote and the areas with a high number of residents with higher education and professional occupations.  On the contrary, there seem to be a positive correlation between the areas with a higher proportion of no educated residents and the leave vote. 

*Don't forget: Correlation DOES NOT imply causation!!

But wait, we can understand better the data if we plot a correlation matrix.  To do that we need the package `GGally` and the package `ggplot2`:
```{r}
library("ggplot2")
library("GGally")
ggpairs(datacor) 

```

<!-- ![alt text](https://github.com/school-of-data/r-consortium-proposal/blob/master/material/lessons/Rplot.png) -->

Now you have some interesting insights and can start writing your story! But wait! how about we try some nice visualizations for the data?

> **Let's review the commands we learned in this step:**
- ....
-.....
...



## Visualize
<!-- https://freakonometrics.hypotheses.org/49832 helped me here -->
```{r}
## remeber to install all packages
## with install.packages()
library("raster")
library("maptools")
library("broom")
library("ggplot2")
library("gpclib")
gpclibPermit()

## get the spatial data for Great Britain (GBR)
gbr_sp <- getData("GADM", country = "GBR", level = 2)

## convert the data to a data frame we can work with
gbr <- fortify(gbr_sp, region = "HASC_2")
str(gbr)

## Our data set has no detailed info on the districts in Northern Ireland
## so we have to change the HASC code (gbr$id) for these districts to just
## "Northern Ireland"
# (1) get data from spatial object which contains info on HASC code and which
#     of the codes correspond to Northern Ireland
mapdat <- gbr_sp@data
# (2) get the HASC codes that correspond to Northern Ireland
NI_HASC <- mapdat$HASC_2[mapdat$NAME_1 == "Northern Ireland"]
# (3) change the relevant HASC codes in gbr to "Northern Ireland"
table(gbr$id[gbr$id %in% NI_HASC]) 
gbr$id[gbr$id %in% NI_HASC] <- "Northern Ireland"

## Merge the map data with the brexit data for the HASC areas we created earlier
gbr_brexit <- merge(gbr, datahasc, by.x = "id", by.y = "HASC_code", all.x = TRUE)

## For plotting spatial data it is important that the order is correct 
ord <- order(gbr_brexit$order)
gbr_brexit <- gbr_brexit[ord, ]

## Now plot
p1 <- ggplot() +
  geom_polygon(data = gbr_brexit, aes(x = long, y = lat, group = group, fill = perc_remain)) +
  coord_map() + xlim(-10, NA)
p1
```



##

...

...

This was the last step! Are you convinced yet? If not, check out [other scenarios]() or [send us an angry tweet]()

## Introductory steps - Recipes

